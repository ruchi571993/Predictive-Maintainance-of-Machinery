# -*- coding: utf-8 -*-
"""Predictive maintainance of Machines using ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fQEVdsTFSUtMA0AmchXAKIdFpvjBwz00
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler,LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.impute import SimpleImputer
from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold
import matplotlib.pyplot as plt
import seaborn as sns

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

"""# Data Loading"""

downloaded_train = drive.CreateFile({'id':"1Xq0gPPovqWmyVrTu2VHPDpewgGlRkyTW"})  
downloaded_train.GetContentFile('train_machine.csv')

downloaded_train = drive.CreateFile({'id':"1JBPXdfv7mem1s0x4IZelzLMOH9w4zYzR"})   
downloaded_train.GetContentFile('test_machine.csv')

dataset = pd.read_csv('train_machine.csv')

print(dataset.head())
print(dataset.describe())

"""#1. Preprocessing"""

dataset.drop(labels='MachineID', axis=1, inplace=True)

dataset.isnull().sum(axis=0)

columns = dataset.columns
cat_attr = ['MachineModel', 'RecentError', 'MostOccuredError', 'RecentReplacedComp', 'MostReplacedComp', 'RecentRepaired', 'MostRepairedComponent', 'ActionPoint']
#num_attr = ['tenure', 'MonthlyCharges', 'TotalCharges']
num_attr = list(set(columns) - set(cat_attr))

print("The total columns in the dataset are {}".format(len(columns)))
print("The numerical columns in the dataset are {}".format(len(num_attr)))
print("The categorical columns in the dataset are {}".format(len(cat_attr)))

dataset[num_attr] = dataset[num_attr].apply(lambda x: x.astype('float64'))
dataset[cat_attr] = dataset[cat_attr].apply(lambda x: x.astype('category'))
print(dataset.dtypes)

sns.countplot(x='ActionPoint', data=dataset)
plt.show()

lab_enc = LabelEncoder()
lab_enc.fit(dataset['ActionPoint'])
dataset['ActionPoint'] = lab_enc.transform(dataset['ActionPoint'])

dataset['ActionPoint'].value_counts()

labels = dataset['ActionPoint']
print(len(labels))
dataset.drop(labels='ActionPoint', axis=1, inplace=True)

print(list(set(cat_attr)-set(['ActionPoint'])))
print(cat_attr)

dataset_dumm = pd.get_dummies(data=dataset, columns=list(set(cat_attr)-set(['ActionPoint'])))
dataset_dumm.columns

"""# Preprocessing test data"""

test = pd.read_csv('test_machine.csv')

test_machineID = test.MachineID.tolist()
test.drop(labels='MachineID', axis=1, inplace=True)

test_columns = dataset.columns
test_cat_attr = ['MachineModel', 'RecentError', 'MostOccuredError', 'RecentReplacedComp', 'MostReplacedComp', 'RecentRepaired', 'MostRepairedComponent']
#num_attr = ['tenure', 'MonthlyCharges', 'TotalCharges']
test_num_attr = list(set(test_columns) - set(test_cat_attr))

test[test_num_attr] = dataset[test_num_attr].apply(lambda x: x.astype('float64'))
test[test_cat_attr] = dataset[test_cat_attr].apply(lambda x: x.astype('category'))

test_dumm = pd.get_dummies(data=test, columns=test_cat_attr)

"""# 2. Modelling

### ***Approach 1***

1. Gradient Boost 
2. AdaBoost
3. Logistic Regression
4. SVM (radial,linear)
5. SC
6. Random Forest

# Gradient Boost
"""

X_train, X_test, y_train, y_test = train_test_split(dataset_dumm, labels, test_size=0.4, stratify=labels, random_state=0)

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

#Setting up pipeline
steps = [('scaler', StandardScaler()),
         ('SGBT', GradientBoostingClassifier())]
pipeline = Pipeline(steps)

#Setting params grid for hyper param tuning
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=143)
param_grid = {'SGBT__max_depth': [3,5, 6,7,8], 'SGBT__subsample': [0.8, 0.7, 0.6,], 'SGBT__max_features':[0.2, 0.3], 
              'SGBT__n_estimators': [10, 20, 30]}
sgbt_cv = GridSearchCV(pipeline, param_grid=param_grid, cv=kfold)

sgbt_cv.fit(X_train, y_train)

sgbt_cv_best = sgbt_cv.best_estimator_
y_pred = sgbt_cv_best.predict(X_test)

print("Accuracy: {}".format(sgbt_cv_best.score(X_test, y_test)))
print(classification_report(y_test, y_pred))
print("Tuned Model Best Parameters: {}".format(sgbt_cv.best_params_))
print("Tuned Model Best Score: {}".format(sgbt_cv.best_score_))

"""# Ada Boost"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

ada_clf = AdaBoostClassifier(
    DecisionTreeClassifier(max_depth=3), n_estimators=500, learning_rate=0.3, algorithm= 'SAMME', random_state=143)
ada_clf.fit(X_train, y_train)

y_pred_ada = ada_clf.predict(X_test)
print(classification_report(y_test,y_pred_ada))

"""# Logistic Regression"""

logreg = LogisticRegression(multi_class="multinomial", solver="lbfgs")
logreg.fit(X_train, y_train)

y_pred = logreg.predict(X_test)
print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))

confmatrix = confusion_matrix(y_test, y_pred)
print(confmatrix)

print(classification_report(y_test, y_pred))

#Setting params grid for hyper param tuning
kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=143)
param_grid = {'C': [0.05, 0.06, 0.07, 0.08, 0.1]}
log_reg1 = LogisticRegression(multi_class="multinomial", solver="lbfgs", )
log_reg_cv = GridSearchCV(log_reg1, param_grid, cv=kfold, scoring='accuracy')

log_reg_cv.fit(X_train, y_train)
log_reg_cv_best = log_reg_cv.best_estimator_

y_pred = log_reg_cv_best.predict(X_test)
print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(log_reg_cv_best.score(X_test, y_test)))

print(confusion_matrix(y_test, y_pred))

print(classification_report(y_test, y_pred))

"""# SVM"""

from sklearn.svm import SVC
svm = SVC() #default rbf kernel

svm.fit(X_train, y_train)
y_pred = svm.predict(X_test)

print("The accuracy of the svc is {:.2f}".format(svm.score(X_test,y_test)))

print(confusion_matrix(y_test, y_pred))

print(classification_report(y_test, y_pred))

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

#Setting up pipeline
steps = [('scaler', StandardScaler()),
         ('SVM', SVC())]
pipeline = Pipeline(steps)

#Setting params grid for hyper param tuning
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=143)
param_grid = {'SVM__C': [50, 60, 70, 80, 90], 'SVM__gamma': [0.008, 0.003, 0.001], 'SVM__kernel':['rbf'], 
              'SVM__class_weight':[{0:0.3, 1:2, 2:0.6}]}
svm_grid = GridSearchCV(pipeline, param_grid=param_grid, cv=kfold)

svm_grid.fit(X_train, y_train)

svm_cv_best = svm_grid.best_estimator_
y_pred = svm_cv_best.predict(X_test)

print("Accuracy: {}".format(svm_cv_best.score(X_test, y_test)))
print(classification_report(y_test, y_pred))
print("Tuned Model Best Parameters: {}".format(svm_grid.best_params_))
print("Tuned Model Best Score: {}".format(svm_grid.best_score_))

"""# Linear SVM"""

from sklearn.svm import LinearSVC

#linear_svm = LinearSVC(random_state=143, C=10) #OneVsRestClassifier()
steps_linear = [('scaler', StandardScaler()),
         ('SVM_linear', LinearSVC())]
pipeline_linear = Pipeline(steps_linear)

#Setting params grid for hyper param tuning
kfold = StratifiedKFold(n_splits=6, shuffle=True, random_state=143)
param_grid_linear = {'SVM__C': [20, 30]}
linear_svm = GridSearchCV(pipeline, param_grid=param_grid_linear, cv=kfold)

linear_svm.fit(X_train, y_train)

y_pred_linear = linear_svm.predict(X_test)

print("Accuracy: {}".format(linear_svm.score(X_test, y_test)))
print(classification_report(y_test, y_pred_linear))
print("Tuned Model Best Parameters: {}".format(linear_svm.best_params_))
print("Tuned Model Best Score: {}".format(linear_svm.best_score_))

"""# SG Classifier"""

from sklearn.linear_model import SGDClassifier

steps_sgc = [('scaler', StandardScaler()),
         ('SGC', SGDClassifier(n_jobs=-1, random_state=143))]
pipeline_sgc = Pipeline(steps_sgc)

#Setting params grid for hyper param tuning
kfold = StratifiedKFold(n_splits=6, shuffle=True, random_state=143)
param_grid_sgc = {'SGC__loss':['modified_huber'], 'SGC__penalty': ['elasticnet'], 'SGC__alpha': [0.2,0.3],
                     'SGC__l1_ratio': [0.1], 'SGC__max_iter': [70], 'SGC__learning_rate': ['optimal'], 
                     'SGC__eta0': [0.1], 'SGC__class_weight': [{0:0.3, 1:3, 2:0.6}]}
sgc_cv = GridSearchCV(pipeline_sgc, param_grid=param_grid_sgc, cv=kfold)

sgc_cv.fit(X_train, y_train)
sgc_cv_best = sgc_cv.best_estimator_
y_pred_sgc = sgc_cv_best.predict(X_test)

print("Accuracy: {}".format(sgc_cv_best.score(X_test, y_test)))
print(classification_report(y_test, y_pred_sgc))
print("Tuned Model Best Parameters: {}".format(sgc_cv.best_params_))
print("Tuned Model Best Score: {}".format(sgc_cv.best_score_))

"""# Random Forest"""

from sklearn.ensemble import RandomForestClassifier

rf_grid = RandomForestClassifier(n_estimators=10, criterion='entropy', max_depth=5, n_jobs=-1, random_state=153, verbose=1,class_weight=None)

rf_grid.fit(X_train,y_train)

y_preds = rf_grid.predict(X_test)
print(rf_grid.score(X_test,y_test))
print(classification_report(y_test, y_preds))

importances_rf = pd.Series(rf_grid.feature_importances_,index = X_train.columns)
# Sort importances_rf
sorted_importances_rf = importances_rf.sort_values()
# Make a horizontal bar plot
sorted_importances_rf.plot(kind='barh',color='lightgreen')
fig_size = plt.rcParams["figure.figsize"]
fig_size[0] = 20
fig_size[1] = 30
plt.rcParams["figure.figsize"] = fig_size
#plt.figure(figsize=(160,500))
plt.show()

# using variable imp features
sorted_importances_rf.sort_values(ascending=False, inplace=True)
print(sorted_importances_rf[:10])
print(type(sorted_importances_rf))

imp_features = list(sorted_importances_rf.index.values[:12])
dataset_imp = dataset_dumm[imp_features]
dataset_imp.head(5)

X_train, X_test, y_train, y_test = train_test_split(dataset_imp, labels, test_size=0.4, stratify=labels, random_state=1234)

rf_imp = RandomForestClassifier(n_estimators=20, criterion='entropy', max_depth=5, random_state=153, class_weight={0:0.3, 1:2, 2:.7})

rf_imp.fit(X_train,y_train)
y_preds = rf_imp.predict(X_test)

print(rf_imp.score(X_test,y_test))
print(classification_report(y_test, y_preds))

"""### ***Approach 2***

# Clustering and Stacking

splitting data into 3 clusters with abundant data
"""

dataset_dumm['target'] = labels

imbal_data = dataset_dumm[dataset_dumm.target == 1]
abn_data = dataset_dumm[dataset_dumm.target != 1]

abn_target = abn_data['target']
abn_data.drop('target', axis=1, inplace=True)

std_scale = StandardScaler()
abn_data_std = std_scale.fit_transform(abn_data)

from sklearn.cluster import KMeans
clt = KMeans(n_clusters=3, random_state=0)

clt_model = clt.fit(abn_data)

clt_labels = clt_model.labels_
abn_data['clt_labels'] = clt_labels

clt0_data = abn_data[abn_data['clt_labels'] == 0]
clt1_data = abn_data[abn_data['clt_labels'] == 1]
clt2_data = abn_data[abn_data['clt_labels'] == 2]

imbal_data['clt_labels'] = imbal_data['target']
imbal_data.drop('target', axis=1, inplace=True)
imbal_data['clt_labels'].replace(1, 3,inplace=True)

"""### Preparing dataset1"""

dataset1 = pd.concat([clt0_data, imbal_data], axis=0)
dataset1.shape

dataset1['clt_labels'] = dataset1['clt_labels'].map({0: 0, 3: 1})

dataset1 = dataset1.sample(frac=1).reset_index(drop=True)

"""### Preparing dataset2"""

dataset2 = pd.concat([clt1_data, imbal_data], axis=0)
dataset2.shape

dataset2['clt_labels'] = dataset2['clt_labels'].map({1: 0, 3: 1})

dataset2 = dataset2.sample(frac=1).reset_index(drop=True)

"""### Preparing dataset3"""

dataset3 = pd.concat([clt2_data, imbal_data], axis=0)
dataset3.shape

dataset3['clt_labels'] = dataset3['clt_labels'].map({2: 0, 3: 1})

dataset3 = dataset3.sample(frac=1).reset_index(drop=True)

labels1 = dataset1['clt_labels']
labels2 = dataset2['clt_labels']
labels3 = dataset3['clt_labels']

print("The len of labels1 is ", len(labels1))
print("The len of labels2 is ", len(labels2))
print("The len of labels3 is ", len(labels3))

dataset1.drop('clt_labels', axis=1, inplace=True)
dataset2.drop('clt_labels', axis=1, inplace=True)
dataset3.drop('clt_labels', axis=1, inplace=True)

"""# Model Building using approach 2

#SVM

### Dataest 1
"""

X_train1, X_test1, y_train1, y_test1 = train_test_split(dataset1, labels1, test_size=0.2, stratify=labels1, random_state=0) #0.4

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.svm import LinearSVC

#Setting up pipeline
steps = [('scaler', StandardScaler()),
         ('SVM', SVC())] #
pipeline = Pipeline(steps)

#Setting params grid for hyper param tuning
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=143)
param_grid = {'SVM__C': [40,50], 'SVM__gamma': [0.008, 0.003, 0.001], 'SVM__kernel':['rbf'],
             'SVM__class_weight':[{0:0.3, 1:1}]} #'SVM__class_weight':[{0:0.3, 1:2, 2:0.6}]
svm_grid = GridSearchCV(pipeline, param_grid=param_grid, cv=kfold)

svm_grid.fit(X_train1, y_train1)

svm_cv_best = svm_grid.best_estimator_
y_pred1 = svm_cv_best.predict(X_test1)

print("Accuracy: {}".format(svm_cv_best.score(X_test1, y_test1)))
print(classification_report(y_test1, y_pred1))
print("Tuned Model Best Parameters: {}".format(svm_grid.best_params_))
print("Tuned Model Best Score: {}".format(svm_grid.best_score_))

"""### Dataset 3"""

from sklearn.svm import LinearSVC

X_train3, X_test3, y_train3, y_test3 = train_test_split(dataset3, labels3, test_size=0.2, stratify=labels3, random_state=0)

steps_linear = [('scaler', StandardScaler()),
         ('SVM_linear', LinearSVC())] 
pipeline_linear = Pipeline(steps_linear)

#Setting params grid for hyper param tuning
kfold = StratifiedKFold(n_splits=6, shuffle=True, random_state=143)
param_grid_linear = {'SVM__C': [50]}
linear_svm = GridSearchCV(pipeline, param_grid=param_grid_linear, cv=kfold)

linear_svm.fit(X_train3, y_train3)

y_pred_linear3 = linear_svm.predict(X_test3)

print("Accuracy: {}".format(linear_svm.score(X_test3, y_test3)))
print(classification_report(y_test3, y_pred_linear3))
print("Tuned Model Best Parameters: {}".format(linear_svm.best_params_))
print("Tuned Model Best Score: {}".format(linear_svm.best_score_))

"""### Dataset 2"""

X_train2, X_test2, y_train2, y_test2 = train_test_split(dataset2, labels2, test_size=0.2, stratify=labels2, random_state=0) #0.4

from sklearn.linear_model import SGDClassifier
steps_sgc = [('scaler', StandardScaler()),
         ('SGC', SGDClassifier(random_state=143))]
pipeline_sgc = Pipeline(steps_sgc)

#Setting params grid for hyper param tuning
kfold = StratifiedKFold(n_splits=6, shuffle=True, random_state=143)
param_grid_sgc = {'SGC__loss':['modified_huber'], 'SGC__penalty': ['elasticnet'], 'SGC__alpha': [0.2,0.3],
                     'SGC__l1_ratio': [0.1], 'SGC__max_iter': [70], 'SGC__learning_rate': ['optimal'], 
                     'SGC__eta0': [0.1], 'SGC__class_weight': [{0:0.3, 1:1}]}
sgc_cv = GridSearchCV(pipeline_sgc, param_grid=param_grid_sgc, cv=kfold)

sgc_cv.fit(X_train2, y_train2)

sgc_cv_best = sgc_cv.best_estimator_
y_pred_sgc2 = sgc_cv_best.predict(X_test2)

print("Accuracy: {}".format(sgc_cv_best.score(X_test2, y_test2)))
print(classification_report(y_test2, y_pred_sgc2))
print("Tuned Model Best Parameters: {}".format(sgc_cv.best_params_))
print("Tuned Model Best Score: {}".format(sgc_cv.best_score_))

"""### Model1  Predictions"""

test_preds1 = svm_cv_best.predict(test_dumm)

"""### Model2 predictions"""

test_preds2 = sgc_cv_best.predict(test_dumm)

"""### Model 3 predictions"""

test_preds3 = linear_svm.predict(test_dumm)

test_preds_dict = {'Testpreds1': test_preds1, 'Testpreds2': test_preds2, 'Testpreds3': test_preds3}
test_preds_df = pd.DataFrame(test_preds_dict)
test_preds_df.head(5)

"""### Combining all models"""

result_preds = test_preds_df.mode(axis=1)
result_preds.columns = ['result']
result_preds.head(5)

result_preds['result'] = result_preds['result'].map({0: 'ComponentRepair', 1: 'ComponentReplacement'})

result_preds['result'].value_counts()

"""The SG classifier and model using approach 2 are the best models as compared to rest"""